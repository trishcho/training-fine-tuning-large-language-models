{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb6rdwlCsCGt"
   },
   "source": [
    "# Comparing Llama 3.1 vs. GPT-4o mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTzBUFWQ-OWj"
   },
   "source": [
    "In this notebook you will use the Meta's Llama 3.1 and OpenAI's GPT-4o mini to solve:\n",
    "\n",
    "- Task 1: Zero-shot Classification\n",
    "- Task 2: Few-shot Classification\n",
    "- Task 3: Coding Tasks - Python\n",
    "- Task 4: Coding Tasks - SQL\n",
    "- Task 5: Information Extraction\n",
    "- Task 6: Closed-Domain Question Answering\n",
    "- Task 7: Open-Domain Question Answering\n",
    "- Task 8: Document Summarization\n",
    "- Task 9: Transformation\n",
    "- Task 10: Translation\n",
    "\n",
    "\n",
    "\n",
    "___Created By: Dipanjan (DJ)___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiwGjVWK4q6F"
   },
   "source": [
    "## Load OpenAI API Credentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ryheOZuXxa41",
    "outputId": "4c14be03-20f6-4bf5-8887-169f2adc198a"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API Key:  ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "openai_key = getpass(\"Enter your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kDe44J0N0NcC"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from IPython.display import HTML, Markdown, display\n",
    "\n",
    "openai.api_key = openai_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yA7H416rWpZ-"
   },
   "source": [
    "## Load HuggingFace API Credentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1w_sS9gTWne6",
    "outputId": "aa0a5771-7006-4eee-8eed-252060bcc8e6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098a672b0ee84c768a0c89b08c662a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDWhgxCy5bA6"
   },
   "source": [
    "## Create ChatGPT Completion Access Function\n",
    "\n",
    "This function will use the [Chat Completion API](https://platform.openai.com/docs/api-reference/chat/create) to access ChatGPT for us and return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kA9gVCwK0WKd"
   },
   "outputs": [],
   "source": [
    "def get_completion_gpt(prompt, model=\"gpt-4o-mini\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.0, # degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwazbbBNPDEM"
   },
   "source": [
    "## Create Llama 3.1 Completion Access Function\n",
    "\n",
    "This function will use the transformers [pipeline](https://huggingface.co/docs/transformers/en/main_classes/pipelines) module to download and load LLama 3.1 8B for us and return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c0f900f8e0214e8c8285822d631177b2",
      "034d51e075b3496cac70aa337661a1e0",
      "507a19ba74254128b60bf2ad989fd797",
      "2ebcda8f035e441a84657ece09e7e909",
      "ec69e77dcaf34452a05b715e204115e0",
      "b440fda4273e440eb998da140692001c",
      "720ccfbcc77248a492c4ccd2336140b1",
      "9d8ad2c43f324a4f9e80d2d619fe9294",
      "1fe3ec8ef7974c568bb570109eed81d1",
      "27d8d80d4a1443e7a64e3687e2794bc7",
      "8d1e7f5a1b8241fa9d91ca438bbf08d2"
     ]
    },
    "id": "IUn-4ioRW_vs",
    "outputId": "27ddb5f6-4a99-43dd-b6ba-add7cadc925b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b76e3a025194b868e98eb1aabe7723f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e15638bf8164668be90e457bfd6af74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3494f25efba44f03b0fdd26be1755c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da74da8605d4ff8b872dc119e8f033d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9bcac8e57b54c528047550fa37e334a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b9bf462bd64a2096904de7c62852cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec40b0e10ec4416db5edfbe8499153d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a4fc127a454c67a6cc2733ea134ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983d32335bf44f078c3a78419fa44cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3baf29f90bf24502ba73d1f149fb5e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cdba13fa3b94d25a1e87c4718dd6ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45401876b74849aba582fc78fb64adad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, \n",
    "                                          cache_dir='./llama3')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=dtype,\n",
    "    cache_dir='./llama3',\n",
    ")\n",
    "\n",
    "llama3 = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "k1MbcFIqW56y"
   },
   "outputs": [],
   "source": [
    "def get_completion_llama(prompt, model_pipeline=llama3):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = model_pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=2000\n",
    "    )\n",
    "    return response[0][\"generated_text\"][-1]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TFZjzuGjCOw"
   },
   "source": [
    "## Let's try out the ChatGPT API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "KK-kjmMoi5rO",
    "outputId": "e381db1c-5e27-4bff-a2e9-decf066c245c"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- **Definition and Functionality**: Generative AI refers to a class of artificial intelligence models that can create new content, such as text, images, music, or videos, by learning patterns and structures from existing data. These models, like GPT and DALL-E, utilize techniques such as deep learning and neural networks to generate outputs that mimic human creativity.\n",
       "\n",
       "- **Applications and Impact**: Generative AI has a wide range of applications, including content creation, design, gaming, and personalized experiences. It can enhance productivity, enable new forms of artistic expression, and facilitate innovation across various industries, but it also raises ethical concerns regarding copyright, misinformation, and the authenticity of generated content."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion_gpt(prompt='Explain Generative AI in 2 bullet points')\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lrx7JUzz-rkZ"
   },
   "source": [
    "## Let's try out LLama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "pFZ7T5mhZy14",
    "outputId": "45f6e28d-7180-457a-b906-47c3e244655f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here are 2 bullet points explaining Generative AI:\n",
       "\n",
       "• **Definition**: Generative AI refers to a type of artificial intelligence that can create new, original content such as images, music, videos, text, or even entire worlds, based on patterns and structures learned from existing data. This is achieved through complex algorithms and machine learning models that can generate novel outputs that are often indistinguishable from human-created content.\n",
       "\n",
       "• **Applications**: Generative AI has a wide range of applications across various industries, including art, music, film, gaming, and advertising. It can be used to generate realistic images and videos, create new music and audio tracks, write stories and articles, and even design new products and experiences. Additionally, generative AI can also be used for tasks such as data augmentation, anomaly detection, and personalized content creation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion_llama(prompt='Explain Generative AI in 2 bullet points')\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AeDkpvGDhMGV"
   },
   "source": [
    "## Task 1: Zero-shot Classification\n",
    "\n",
    "This prompt tests an LLM's text classification capabilities by prompting it to classify a piece of text without providing any examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hRbBZB57hT0G"
   },
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    f\"\"\"\n",
    "    Just received the Bluetooth speaker I ordered for beach outings, and it's fantastic.\n",
    "    The sound quality is impressively clear with just the right amount of bass.\n",
    "    It's also waterproof, which tested true during a recent splashing incident.\n",
    "    Though it's compact, the volume can really fill the space.\n",
    "    The price was a bargain for such high-quality sound.\n",
    "    Shipping was also on point, arriving two days early in secure packaging.\n",
    "    \"\"\",\n",
    "    f\"\"\"\n",
    "    Needed a new kitchen blender, but this model has been a nightmare.\n",
    "    It's supposed to handle various foods, but it struggles with anything tougher than cooked vegetables.\n",
    "    It's also incredibly noisy, and the 'easy-clean' feature is a joke; food gets stuck under the blades constantly.\n",
    "    I thought the brand meant quality, but this product has proven me wrong.\n",
    "    Plus, it arrived three days late. Definitely not worth the expense.\n",
    "    \"\"\",\n",
    "    f\"\"\"\n",
    "    I tried to like this book and while the plot was really good, the print quality was so not good\n",
    "    \"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZwPaViatl7f",
    "outputId": "3bec7089-16e1-4cd9-bee8-5d28b6ccc8bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "responses = {\n",
    "    'llama3.1' : [],\n",
    "    'gpt-4o-mini' : []\n",
    "}\n",
    "\n",
    "for review in reviews:\n",
    "  prompt = f\"\"\"\n",
    "              Act as a product review analyst.\n",
    "              Given the following review,\n",
    "              Display the overall sentiment for the review as only one of the following:\n",
    "              Positive, Negative OR Neutral\n",
    "\n",
    "              Just give me the sentiment only.\n",
    "\n",
    "              ```{review}```\n",
    "              \"\"\"\n",
    "  response = get_completion_llama(prompt)\n",
    "  responses['llama3.1'].append(response)\n",
    "  response = get_completion_gpt(prompt)\n",
    "  responses['gpt-4o-mini'].append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "EdUFkKAmtmBj",
    "outputId": "0d3360eb-e0e3-438d-82a0-34a3a87e5128"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llama3.1</th>\n",
       "      <th>gpt-4o-mini</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   llama3.1 gpt-4o-mini\n",
       "0  Positive    Positive\n",
       "1  Negative    Negative\n",
       "2  Negative    Negative"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WC6aycZ9Qe3"
   },
   "source": [
    "## Task 2: Few-shot Classification\n",
    "\n",
    "This prompt tests an LLM's text classification capabilities by prompting it to classify a piece of text by providing a few examples of inputs and outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQZdygfUoXGT",
    "outputId": "22296cb8-d451-4b68-b4d5-3c581591d331"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "responses = {\n",
    "    'llama3.1' : [],\n",
    "    'gpt-4o-mini' : []\n",
    "}\n",
    "\n",
    "\n",
    "for review in reviews:\n",
    "  prompt = f\"\"\"\n",
    "              Act as a product review analyst.\n",
    "              Given the following review,\n",
    "              Display only the sentiment for the review:\n",
    "\n",
    "              Try to classify it by using the following examples as a reference:\n",
    "\n",
    "              Review: Just received the Laptop I ordered for work, and it's amazing.\n",
    "              Sentiment: 😊\n",
    "\n",
    "              Review: Needed a new mechanical keyboard, but this model has been totally disappointing.\n",
    "              Sentiment: 😡\n",
    "\n",
    "              Review: ```{review}```\n",
    "              Sentiment:\n",
    "              \"\"\"\n",
    "  response = get_completion_llama(prompt)\n",
    "  responses['llama3.1'].append(response)\n",
    "  response = get_completion_gpt(prompt)\n",
    "  responses['gpt-4o-mini'].append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "id": "OCsMzs3QodqR",
    "outputId": "12e44cbb-2335-4813-c154-724b5504bbed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llama3.1</th>\n",
       "      <th>gpt-4o-mini</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Based on the reference examples, I would classify the sentiment of the review as:\\n\\n😊</td>\n",
       "      <td>😊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Based on the given examples and the review, I would classify the sentiment as:\\n\\n😡\\n\\nThe review expresses several negative points about the product, including its performance, noise level, and design flaws. The reviewer also mentions that the product did not meet their expectations and was not worth the expense. These negative comments and the use of phrases like \"nightmare\", \"struggles\", and \"joke\" contribute to a strong negative sentiment.</td>\n",
       "      <td>😡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Based on the given examples and the review:\\n\\n``` \\nI tried to like this book and while the plot was really good, the print quality was so not good\\n```\\n\\nThe sentiment would be: 😐\\n\\nThis is because the reviewer tried to like the book, indicating a positive inclination, but then mentioned a major drawback (print quality), which is a negative aspect. The overall sentiment is neutral, as the reviewer is torn between the two aspects.</td>\n",
       "      <td>😐</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                          llama3.1  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                           Based on the reference examples, I would classify the sentiment of the review as:\\n\\n😊   \n",
       "1  Based on the given examples and the review, I would classify the sentiment as:\\n\\n😡\\n\\nThe review expresses several negative points about the product, including its performance, noise level, and design flaws. The reviewer also mentions that the product did not meet their expectations and was not worth the expense. These negative comments and the use of phrases like \"nightmare\", \"struggles\", and \"joke\" contribute to a strong negative sentiment.   \n",
       "2            Based on the given examples and the review:\\n\\n``` \\nI tried to like this book and while the plot was really good, the print quality was so not good\\n```\\n\\nThe sentiment would be: 😐\\n\\nThis is because the reviewer tried to like the book, indicating a positive inclination, but then mentioned a major drawback (print quality), which is a negative aspect. The overall sentiment is neutral, as the reviewer is torn between the two aspects.   \n",
       "\n",
       "  gpt-4o-mini  \n",
       "0           😊  \n",
       "1           😡  \n",
       "2           😐  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEKTyfWf9cxY"
   },
   "source": [
    "## Task 3: Coding Tasks - Python\n",
    "\n",
    "This prompt tests an LLM's capabilities for generating python code based on various tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6xawr-Co9b0t"
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Act as an expert in generating python code\n",
    "\n",
    "Your task is to generate python code\n",
    "to explain how to scale data for a ML problem.\n",
    "Focus on just scaling and nothing else.\n",
    "Keep into account key operations we should do on the data\n",
    "to prevent data leakage before scaling.\n",
    "Keep the code and answer concise.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBzNaSpL9LoZ"
   },
   "source": [
    "Trying with Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719
    },
    "id": "2Cenat0G9-Bw",
    "outputId": "e1e61fae-c256-4ef0-fbe3-3f8d463320a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Scaling Data for Machine Learning**\n",
       "=====================================\n",
       "\n",
       "To scale data for a machine learning problem, we'll use the `StandardScaler` from scikit-learn. Here's a concise example of how to do it:\n",
       "\n",
       "```python\n",
       "# Import necessary libraries\n",
       "from sklearn.preprocessing import StandardScaler\n",
       "import pandas as pd\n",
       "\n",
       "# Load your dataset into a pandas DataFrame\n",
       "data = pd.read_csv('your_data.csv')\n",
       "\n",
       "# Split data into features (X) and target (y)\n",
       "X = data.drop('target_column', axis=1)  # replace 'target_column' with your actual target column\n",
       "y = data['target_column']\n",
       "\n",
       "# Split data into training and testing sets\n",
       "from sklearn.model_selection import train_test_split\n",
       "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
       "\n",
       "# Create a StandardScaler object\n",
       "scaler = StandardScaler()\n",
       "\n",
       "# Fit the scaler to the training data and transform both the training and testing data\n",
       "X_train_scaled = scaler.fit_transform(X_train)\n",
       "X_test_scaled = scaler.transform(X_test)\n",
       "\n",
       "# Now you can use the scaled data for your machine learning model\n",
       "```\n",
       "\n",
       "**Key Operations:**\n",
       "\n",
       "1. **Split data into features (X) and target (y)**: This is crucial to prevent data leakage.\n",
       "2. **Split data into training and testing sets**: Again, this prevents data leakage.\n",
       "3. **Fit the scaler to the training data**: This is where the scaler learns the mean and standard deviation of the training data.\n",
       "4. **Transform both the training and testing data**: This applies the scaling to both datasets.\n",
       "\n",
       "By following these steps, you'll ensure that your data is properly scaled for your machine learning model."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion_llama(prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb9P3qGL9lxA"
   },
   "source": [
    "Trying with GPT-4o mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "id": "LVjvz6Ow-OXd",
    "outputId": "8b8aaf4b-9729-4cbc-b685-0a825f41261d"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Below is a concise Python code snippet that demonstrates how to scale data for a machine learning problem while preventing data leakage. We'll use `StandardScaler` from `sklearn` as an example of scaling.\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "from sklearn.model_selection import train_test_split\n",
       "from sklearn.preprocessing import StandardScaler\n",
       "\n",
       "# Sample data\n",
       "data = pd.DataFrame({\n",
       "    'feature1': [1, 2, 3, 4, 5],\n",
       "    'feature2': [10, 20, 30, 40, 50],\n",
       "    'target': [0, 1, 0, 1, 0]\n",
       "})\n",
       "\n",
       "# Step 1: Split the data into training and testing sets\n",
       "X = data[['feature1', 'feature2']]\n",
       "y = data['target']\n",
       "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
       "\n",
       "# Step 2: Initialize the scaler\n",
       "scaler = StandardScaler()\n",
       "\n",
       "# Step 3: Fit the scaler only on the training data\n",
       "X_train_scaled = scaler.fit_transform(X_train)\n",
       "\n",
       "# Step 4: Transform the test data using the fitted scaler\n",
       "X_test_scaled = scaler.transform(X_test)\n",
       "\n",
       "# X_train_scaled and X_test_scaled are now scaled without data leakage\n",
       "```\n",
       "\n",
       "### Key Operations:\n",
       "1. **Data Splitting**: Always split your data into training and testing sets before scaling to prevent data leakage.\n",
       "2. **Fitting the Scaler**: Fit the scaler only on the training data.\n",
       "3. **Transforming the Test Data**: Use the fitted scaler to transform the test data, ensuring that the test set remains unseen during the training phase."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion_gpt(prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JoSF5wb_Imw"
   },
   "source": [
    "## Task 4: Coding Tasks - SQL\n",
    "\n",
    "This prompt tests an LLM's capabilities for generating SQL code based on various tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4XKGZIG3_Tjl"
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Act as an expert in generating SQL code.\n",
    "\n",
    "Understand the following schema of the database tables carefully:\n",
    "Table departments, columns = [DepartmentId, DepartmentName]\n",
    "Table employees, columns = [EmployeeId, EmployeeName, DepartmentId]\n",
    "Table salaries, columns = [EmployeeId, Salary]\n",
    "\n",
    "Create a MySQL query for the employee with the 2nd highest salary in the 'IT' Department.\n",
    "Output should have EmployeeId, EmployeeName, DepartmentName, Salary\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgkJOZd_-ymF"
   },
   "source": [
    "Trying with Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 956
    },
    "id": "jZm4ZuoB9xPd",
    "outputId": "5718b75f-137e-4334-dcff-cef129fefd81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "To solve this problem, we can use a subquery to first find the maximum and second maximum salaries in the 'IT' department, and then join the employees and departments tables to get the desired output.\n",
       "\n",
       "Here's the MySQL query:\n",
       "\n",
       "```sql\n",
       "SELECT \n",
       "    e.EmployeeId,\n",
       "    e.EmployeeName,\n",
       "    d.DepartmentName,\n",
       "    s.Salary\n",
       "FROM \n",
       "    employees e\n",
       "JOIN \n",
       "    departments d ON e.DepartmentId = d.DepartmentId\n",
       "JOIN \n",
       "    salaries s ON e.EmployeeId = s.EmployeeId\n",
       "WHERE \n",
       "    d.DepartmentName = 'IT'\n",
       "    AND s.Salary IN (\n",
       "        SELECT \n",
       "            DISTINCT Salary\n",
       "        FROM \n",
       "            salaries\n",
       "        WHERE \n",
       "            EmployeeId IN (\n",
       "                SELECT \n",
       "                    EmployeeId\n",
       "                FROM \n",
       "                    employees\n",
       "                WHERE \n",
       "                    DepartmentName = 'IT'\n",
       "            )\n",
       "        ORDER BY \n",
       "            Salary DESC\n",
       "        LIMIT 2\n",
       "    );\n",
       "```\n",
       "\n",
       "However, the query above is not the most efficient way to solve this problem. We can optimize it by using a subquery to get the second highest salary in the 'IT' department, and then join the employees and departments tables to get the desired output.\n",
       "\n",
       "Here's the optimized query:\n",
       "\n",
       "```sql\n",
       "SELECT \n",
       "    e.EmployeeId,\n",
       "    e.EmployeeName,\n",
       "    d.DepartmentName,\n",
       "    s.Salary\n",
       "FROM \n",
       "    employees e\n",
       "JOIN \n",
       "    departments d ON e.DepartmentId = d.DepartmentId\n",
       "JOIN \n",
       "    salaries s ON e.EmployeeId = s.EmployeeId\n",
       "WHERE \n",
       "    d.DepartmentName = 'IT'\n",
       "    AND s.Salary = (\n",
       "        SELECT \n",
       "            DISTINCT Salary\n",
       "        FROM \n",
       "            salaries\n",
       "        WHERE \n",
       "            EmployeeId IN (\n",
       "                SELECT \n",
       "                    EmployeeId\n",
       "                FROM \n",
       "                    employees\n",
       "                WHERE \n",
       "                    DepartmentName = 'IT'\n",
       "            )\n",
       "        ORDER BY \n",
       "            Salary DESC\n",
       "        LIMIT 1 OFFSET 1\n",
       "    );\n",
       "```\n",
       "\n",
       "However, MySQL does not support the `LIMIT` clause in a subquery, so we need to use a different approach. We can use a variable to store the maximum salary and then select the employee with the second highest salary.\n",
       "\n",
       "Here's the query using a variable:\n",
       "\n",
       "```sql\n",
       "SET @max_salary = NULL;\n",
       "\n",
       "SELECT \n",
       "    e.EmployeeId,\n",
       "    e.EmployeeName,\n",
       "    d.DepartmentName,\n",
       "    s.Salary\n",
       "FROM \n",
       "    employees e\n",
       "JOIN \n",
       "    departments d ON e.DepartmentId = d.DepartmentId\n",
       "JOIN \n",
       "    salaries s ON e.EmployeeId = s.EmployeeId\n",
       "WHERE \n",
       "    d.DepartmentName = 'IT'\n",
       "    AND s.Salary = (\n",
       "        SELECT \n",
       "            DISTINCT Salary\n",
       "        FROM \n",
       "            salaries\n",
       "        WHERE \n",
       "            EmployeeId IN (\n",
       "                SELECT \n",
       "                    EmployeeId\n",
       "                FROM \n",
       "                    employees\n",
       "                WHERE \n",
       "                    DepartmentName = 'IT'\n",
       "            )\n",
       "        ORDER BY \n",
       "            Salary DESC\n",
       "        LIMIT 1 OFFSET 1\n",
       "    )\n",
       "    OR s.Salary = (\n",
       "        SELECT \n",
       "            @max_salary := MAX(Salary)\n",
       "        FROM \n",
       "            salaries\n",
       "        WHERE \n",
       "            EmployeeId IN (\n",
       "                SELECT \n",
       "                    EmployeeId\n",
       "                FROM \n",
       "                    employees\n",
       "                WHERE \n",
       "                    DepartmentName = 'IT'\n",
       "            )\n",
       "    );\n",
       "```\n",
       "\n",
       "However, the query above is still not the most efficient way to solve this problem. We can optimize it by using a subquery to get the second highest salary in the 'IT' department, and then join the employees and departments tables to get the desired output.\n",
       "\n",
       "Here's the optimized query using a subquery:\n",
       "\n",
       "```sql\n",
       "SELECT \n",
       "    e.EmployeeId,\n",
       "    e.EmployeeName,\n",
       "    d.DepartmentName,\n",
       "    s.Salary\n",
       "FROM \n",
       "    employees e\n",
       "JOIN \n",
       "    departments d ON e.DepartmentId = d.DepartmentId\n",
       "JOIN \n",
       "    salaries s ON e.EmployeeId = s.EmployeeId\n",
       "WHERE \n",
       "    d.DepartmentName = 'IT'\n",
       "    AND s.Salary = (\n",
       "        SELECT \n",
       "            Salary\n",
       "        FROM \n",
       "            (\n",
       "                SELECT \n",
       "                    Salary,\n",
       "                    DENSE_RANK() OVER (ORDER BY Salary DESC) AS rank\n",
       "                FROM \n",
       "                    salaries\n",
       "                WHERE \n",
       "                    EmployeeId IN (\n",
       "                        SELECT \n",
       "                            EmployeeId\n",
       "                        FROM \n",
       "                            employees\n",
       "                        WHERE \n",
       "                            DepartmentName = 'IT'\n",
       "                    )\n",
       "            ) t\n",
       "        WHERE \n",
       "            rank = 2\n",
       "    );\n",
       "```\n",
       "\n",
       "This query uses a subquery to get the second highest salary in the 'IT' department, and then joins the employees and departments tables to get the desired output. This is the most efficient way to solve this problem."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion_llama(prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGlVPiI59xPe"
   },
   "source": [
    "Trying with GPT-4o mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "g-8ohbif9xPe",
    "outputId": "3460aace-ea38-41d6-b564-8aa51b2bbe5a"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To retrieve the employee with the 2nd highest salary in the 'IT' department, you can use a combination of subqueries and joins. Here’s the SQL query that accomplishes this:\n",
       "\n",
       "```sql\n",
       "SELECT e.EmployeeId, e.EmployeeName, d.DepartmentName, s.Salary\n",
       "FROM employees e\n",
       "JOIN departments d ON e.DepartmentId = d.DepartmentId\n",
       "JOIN salaries s ON e.EmployeeId = s.EmployeeId\n",
       "WHERE d.DepartmentName = 'IT'\n",
       "ORDER BY s.Salary DESC\n",
       "LIMIT 1 OFFSET 1;\n",
       "```\n",
       "\n",
       "### Explanation:\n",
       "1. **Joins**: The query joins the `employees`, `departments`, and `salaries` tables to get the necessary information.\n",
       "2. **WHERE Clause**: It filters the results to include only employees from the 'IT' department.\n",
       "3. **ORDER BY**: The results are ordered by `Salary` in descending order to get the highest salaries first.\n",
       "4. **LIMIT and OFFSET**: The `LIMIT 1 OFFSET 1` clause is used to skip the highest salary (offset 1) and return the next one (limit 1), which corresponds to the 2nd highest salary.\n",
       "\n",
       "This query will return the `EmployeeId`, `EmployeeName`, `DepartmentName`, and `Salary` of the employee with the 2nd highest salary in the 'IT' department."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion_gpt(prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mck_SlcM3bG"
   },
   "source": [
    "## Task 5: Information Extraction\n",
    "\n",
    "This prompt tests an LLM's capabilities for extracting and analyzing key entities from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "UxciK2wsIw3U"
   },
   "outputs": [],
   "source": [
    "clinical_note = \"\"\"\n",
    "60-year-old man in NAD with a h/o CAD, DM2, asthma, pharyngitis, SBP,\n",
    "and HTN on altace for 8 years awoke from sleep around 1:00 am this morning\n",
    "with a sore throat and swelling of the tongue.\n",
    "He came immediately to the ED because he was having difficulty swallowing and\n",
    "some trouble breathing due to obstruction caused by the swelling.\n",
    "He did not have any associated SOB, chest pain, itching, or nausea.\n",
    "He has not noticed any rashes.\n",
    "He says that he feels like it is swollen down in his esophagus as well.\n",
    "He does not recall vomiting but says he might have retched a bit.\n",
    "In the ED he was given 25mg benadryl IV, 125 mg solumedrol IV,\n",
    "and pepcid 20 mg IV.\n",
    "Family history of CHF and esophageal cancer (father).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "GcazKzm0JGKc"
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Act as an expert in analyzing and understanding clinical doctor notes in healthcare.\n",
    "Extract all symptoms only from the clinical note below in triple backticks.\n",
    "\n",
    "Differentiate between symptoms that are present vs. absent.\n",
    "Give me the probability (high/ medium/ low) of how sure you are about the result.\n",
    "Add a note on the probabilities and why you think so.\n",
    "\n",
    "Output as a markdown table with the following columns,\n",
    "all symptoms should be expanded and no acronyms unless you don't know:\n",
    "\n",
    "Symptoms | Present/Denies | Probability.\n",
    "\n",
    "\n",
    "Also expand the acronyms in the note including symptoms and other medical terms.\n",
    "Do not leave out any acronym related to healthcare.\n",
    "\n",
    "Output that also as a separate appendix table in Markdown with the following columns,\n",
    "\n",
    "Acronym | Expanded Term\n",
    "\n",
    "Clinical Note:\n",
    "```{clinical_note}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWTsA46M-0b6"
   },
   "source": [
    "Trying with Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "h3n3ILOa-G6r",
    "outputId": "712ce781-de51-4bf9-f837-fcd34f073ed1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Symptoms Table**\n",
       "\n",
       "| Symptoms | Present/Denies | Probability |\n",
       "| --- | --- | --- |\n",
       "| Sore throat | Present | High |\n",
       "| Swelling of the tongue | Present | High |\n",
       "| Difficulty swallowing | Present | High |\n",
       "| Trouble breathing | Present | High |\n",
       "| Obstruction caused by swelling | Present | High |\n",
       "| Shortness of breath (SOB) | Absent | Medium |\n",
       "| Chest pain | Absent | Medium |\n",
       "| Itching | Absent | Low |\n",
       "| Nausea | Absent | Low |\n",
       "| Rashes | Absent | Low |\n",
       "| Vomiting | Absent | Low |\n",
       "| Retching | Present | Medium |\n",
       "| Swelling in the esophagus | Present | High |\n",
       "\n",
       "**Note on Probabilities:**\n",
       "\n",
       "I have assigned high probability to symptoms that are directly mentioned in the clinical note, such as sore throat, swelling of the tongue, difficulty swallowing, and trouble breathing. These symptoms are explicitly stated by the patient and are consistent with the patient's presentation.\n",
       "\n",
       "I have assigned medium probability to symptoms that are partially mentioned or have some ambiguity, such as shortness of breath (SOB) and retching. The patient does not explicitly deny SOB, but it is not mentioned as a symptom. The patient also mentions retching, but it is not clear if it is a symptom or a consequence of the swelling.\n",
       "\n",
       "I have assigned low probability to symptoms that are explicitly denied or not mentioned at all, such as itching, nausea, and rashes.\n",
       "\n",
       "**Appendix Table: Expanded Acronyms**\n",
       "\n",
       "| Acronym | Expanded Term |\n",
       "| --- | --- |\n",
       "| NAD | Normal and Duly (Note: In medical context, it's more common to see \"NAD\" as Normal and Duly, but it can also be \"Normal and Duly\" or \"Normal and Drunk\" depending on context) |\n",
       "| CAD | Coronary Artery Disease |\n",
       "| DM2 | Diabetes Mellitus Type 2 |\n",
       "| SBP | Systolic Blood Pressure |\n",
       "| HTN | Hypertension |\n",
       "| SOB | Shortness of Breath |\n",
       "| ED | Emergency Department |\n",
       "| IV | Intravenous |\n",
       "| CHF | Congestive Heart Failure |\n",
       "| H/o | History of (Note: This is a medical abbreviation, but it's not an acronym. It's more common to see \"Hx\" or \"H/O\") |\n",
       "| DM | Diabetes Mellitus (Note: DM is not an acronym, but it's a common abbreviation for Diabetes Mellitus) |\n",
       "| SBP | Systolic Blood Pressure |\n",
       "| HTN | Hypertension |\n",
       "| CAD | Coronary Artery Disease |\n",
       "| DM2 | Diabetes Mellitus Type 2 |\n",
       "| HTN | Hypertension |\n",
       "| CHF | Congestive Heart Failure |\n",
       "| ED | Emergency Department |\n",
       "| IV | Intravenous |\n",
       "| H/o | History of |\n",
       "| SOB | Shortness of Breath |\n",
       "| CAD | Coronary Artery Disease |\n",
       "| HTN | Hypertension |\n",
       "| DM2 | Diabetes Mellitus Type 2 |\n",
       "| SBP | Systolic Blood Pressure |\n",
       "| H/o | History of |\n",
       "| CHF | Congestive Heart Failure |\n",
       "| ED | Emergency Department |\n",
       "| IV | Intravenous |\n",
       "| SOB | Shortness of Breath |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion_llama(prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U065sii-G6s"
   },
   "source": [
    "Trying with GPT-4o mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VOagmJJm-G6s",
    "outputId": "56b9c3b2-70ab-451d-89a2-6e3439e93328"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Symptoms Analysis\n",
       "\n",
       "| Symptoms                                   | Present/Denies | Probability |\n",
       "|--------------------------------------------|----------------|-------------|\n",
       "| Sore throat                                | Present        | High        |\n",
       "| Swelling of the tongue                     | Present        | High        |\n",
       "| Difficulty swallowing                      | Present        | High        |\n",
       "| Trouble breathing                          | Present        | High        |\n",
       "| Shortness of breath                        | Denies         | High        |\n",
       "| Chest pain                                 | Denies         | High        |\n",
       "| Itching                                    | Denies         | High        |\n",
       "| Nausea                                     | Denies         | High        |\n",
       "| Rashes                                      | Denies         | High        |\n",
       "| Swelling in the esophagus                  | Present        | Medium      |\n",
       "| Vomiting                                   | Denies         | Medium      |\n",
       "| Retching                                   | Present        | Medium      |\n",
       "\n",
       "### Note on Probabilities\n",
       "The probabilities are assessed based on the clarity and specificity of the symptoms mentioned in the clinical note. Symptoms that are explicitly stated as present (e.g., sore throat, swelling of the tongue) are given a high probability due to direct mention. Symptoms that are denied (e.g., shortness of breath, chest pain) are also assigned a high probability since they are clearly stated as absent. For symptoms like swelling in the esophagus and retching, the probability is medium because they are somewhat subjective and less clearly defined in the context of the note.\n",
       "\n",
       "### Appendix: Acronyms\n",
       "\n",
       "| Acronym | Expanded Term                          |\n",
       "|---------|----------------------------------------|\n",
       "| NAD     | No Acute Distress                      |\n",
       "| h/o     | History of                             |\n",
       "| CAD     | Coronary Artery Disease                |\n",
       "| DM2     | Type 2 Diabetes Mellitus               |\n",
       "| SBP     | Systolic Blood Pressure                 |\n",
       "| HTN     | Hypertension                           |\n",
       "| IV      | Intravenous                            |\n",
       "| CHF     | Congestive Heart Failure               | \n",
       "| ED      | Emergency Department                   | \n",
       "| mg      | Milligram                              |\n",
       "| solumedrol | Methylprednisolone                  |\n",
       "| pepcid  | Famotidine                             |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion_gpt(prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jucY6L9w14Z"
   },
   "source": [
    "## Task 6: Closed-Domain Question Answering\n",
    "\n",
    "Question Answering (QA) is a natural language processing task which involves generating the desired answer for the given question. Question Answering can be open-domain QA or closed-domain QA depending on whether the LLM is provided with the relevant context or not.\n",
    "\n",
    "In the case of closed-domain QA, a question along with relevant context is given. Here the context is nothing but the relevant text which ideally should have the answer. Just like a RAG workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "8I7ZsP3OxL-n"
   },
   "outputs": [],
   "source": [
    "report = \"\"\"\n",
    "Three quarters (77%) of the population saw an increase in their regular outgoings over the past year,\n",
    "according to findings from our recent consumer survey. In contrast, just over half (54%) of respondents\n",
    "had an increase in their salary, which suggests that the burden of costs outweighing income remains for\n",
    "most. In total, across the 2,500 people surveyed, the increase in outgoings was 18%, three times higher\n",
    "than the 6% increase in income.\n",
    "\n",
    "Despite this, the findings of our survey suggest we have reached a plateau. Looking at savings,\n",
    "for example, the share of people who expect to make regular savings this year is just over 70%,\n",
    "broadly similar to last year. Over half of those saving plan to use some of the funds for residential\n",
    "property. A third are saving for a deposit, and a further 20% for an investment property or second home.\n",
    "\n",
    "But for some, their plans are being pushed back. 9% of respondents stated they had planned to purchase\n",
    "a new home this year but have now changed their mind. While for many the deposit may be an issue,\n",
    "the other driving factor remains the cost of the mortgage, which has been steadily rising the last\n",
    "few years. For those that currently own a property, the survey showed that in the last year,\n",
    "the average mortgage payment has increased from £668.51 to £748.94, or 12%.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "dAPf8hD2xATM"
   },
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "How much has the average mortage payment increased in the last year?\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Using the following context information below please answer the following question\n",
    "to the best of your ability\n",
    "Context:\n",
    "{report}\n",
    "Question:\n",
    "{question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pfa53eC5-2YC"
   },
   "source": [
    "Trying with Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "id": "8TfeP5W5A2zQ",
    "outputId": "69e7fce4-27bb-4ad7-85af-8939fa208f31"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "According to the context information, the average mortgage payment has increased from £668.51 to £748.94, which is an increase of £80.43."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion_llama(prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hD2P_ds1A2zR"
   },
   "source": [
    "Trying with GPT-4o mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "mbkGkiyxA2zR",
    "outputId": "1925a506-7aee-4b45-873f-5aed09b5ed7d"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The average mortgage payment has increased from £668.51 to £748.94 in the last year, which is an increase of £80.43."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion_gpt(prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "A-Dxn9d_xibD"
   },
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "What percentage of people had an increase in salary last year? Show the answer just as a number.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Using the following context information below please answer the following question\n",
    "to the best of your ability\n",
    "Context:\n",
    "{report}\n",
    "Question:\n",
    "{question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uP9LtxAh-3Ol"
   },
   "source": [
    "Trying with Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "id": "E9W0giotA9G9",
    "outputId": "48763a8d-50ff-4e90-858b-ce92bbaa8f29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "54"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion_llama(prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GdLytZ2A9G9"
   },
   "source": [
    "Trying with GPT-4o mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "RbeczVtGA9G9",
    "outputId": "8aee2d64-970c-419d-981c-fd442f47fbc5"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "54"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion_gpt(prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuQlsSmWxyCd"
   },
   "source": [
    "## Task 7: Open-Domain Question Answering\n",
    "\n",
    "Question Answering (QA) is a natural language processing task which involves generating the desired answer for the given question.\n",
    "\n",
    "In the case of open-domain QA, only the question is asked without providing any context or information. Here, the LLM answers the question using the knowledge gained from large volumes of text data during its training. This is basically Zero-Shot QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fV2MP4OdyETp"
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Please answer the following question to the best of your ability\n",
    "Question:\n",
    "What is LangChain?\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "putCc0if-4Iy"
   },
   "source": [
    "Trying with Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "AeISRwfZBJOj",
    "outputId": "6f491803-09ee-4db8-eacb-14f2cfa575c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "LangChain is an open-source Python library that enables developers to build conversational AI applications using a modular, chain-based architecture. It provides a set of building blocks, called \"chains,\" that can be combined to create complex conversational flows, allowing for more natural and context-aware interactions.\n",
       "\n",
       "LangChain's architecture is designed to facilitate the creation of conversational AI systems that can understand and respond to user input in a more human-like way. It achieves this by breaking down the conversation into smaller, manageable components, which can be chained together to create a cohesive and context-aware conversation flow.\n",
       "\n",
       "Some key features of LangChain include:\n",
       "\n",
       "1. **Modular architecture**: LangChain's chain-based architecture allows developers to build conversational AI systems by combining smaller, reusable components.\n",
       "2. **Context-aware conversations**: LangChain enables developers to create conversational flows that are aware of the context and history of the conversation.\n",
       "3. **Natural language understanding**: LangChain provides a range of natural language processing (NLP) tools and techniques to enable developers to build conversational AI systems that can understand user input.\n",
       "4. **Integration with other libraries**: LangChain is designed to work seamlessly with other popular AI and NLP libraries, such as Hugging Face Transformers and spaCy.\n",
       "\n",
       "Overall, LangChain is a powerful tool for building conversational AI applications that can understand and respond to user input in a more natural and context-aware way."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion_llama(prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbOF16TjBJOk"
   },
   "source": [
    "Trying with GPT-4o mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "MlfENVC0BJOk",
    "outputId": "3b3c2836-e73c-4350-ebb1-bba54c8f12ac"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LangChain is a framework designed to facilitate the development of applications that utilize large language models (LLMs). It provides a set of tools and components that help developers build, manage, and deploy applications that can leverage the capabilities of LLMs for various tasks, such as natural language understanding, text generation, and conversational agents.\n",
       "\n",
       "Key features of LangChain include:\n",
       "\n",
       "1. **Modularity**: LangChain allows developers to create modular components that can be easily integrated and reused across different applications.\n",
       "\n",
       "2. **Chain Management**: It supports the creation of \"chains,\" which are sequences of operations that can be executed in a specific order, enabling complex workflows.\n",
       "\n",
       "3. **Integration with External Data Sources**: LangChain can connect with various data sources, APIs, and databases, allowing applications to pull in relevant information to enhance the capabilities of the language model.\n",
       "\n",
       "4. **Prompt Management**: The framework provides tools for managing and optimizing prompts, which are crucial for guiding the behavior of LLMs.\n",
       "\n",
       "5. **Support for Multiple LLMs**: LangChain is designed to work with various language models, making it flexible for developers to choose the best model for their specific use case.\n",
       "\n",
       "Overall, LangChain aims to simplify the process of building applications that harness the power of language models, making it easier for developers to create innovative solutions in areas like chatbots, content generation, and more."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = response = get_completion_gpt(prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "z-J4IqXxghpD"
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Please answer the following question to the best of your ability\n",
    "Question:\n",
    "13.11 or 13.8 which is larger and why?\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4zZMwo8-5TW"
   },
   "source": [
    "Trying with Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "UlxAQg4igwg0",
    "outputId": "8cf31163-a5ae-418d-bfbc-b827ff416774"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The number 13.8 is larger than 13.11.\n",
       "\n",
       "This is because 13.8 has a higher decimal value than 13.11. In a decimal number, the digits to the right of the decimal point represent fractions of a unit. The more digits to the right of the decimal point, the more precise the number is. \n",
       "\n",
       "In this case, 13.8 has a smaller decimal value (0.8) compared to 13.11 (0.11). However, since 0.11 is larger than 0.8, 13.11 is larger than 13.8."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion_llama(prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8lGpT17gwg1"
   },
   "source": [
    "Trying with GPT-4o mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "T3grRw0dgwg1",
    "outputId": "d4e04a7a-ca20-4b6c-8ab2-98055baadfc1"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "13.11 is larger than 13.8. This is because when comparing decimal numbers, you start from the leftmost digit. In this case, both numbers have the same whole number part (13). However, when you compare the tenths place, 1 (in 13.11) is greater than 0 (in 13.8). Therefore, 13.11 is larger than 13.8."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = response = get_completion_gpt(prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnVmXCZAzyux"
   },
   "source": [
    "## Task 8: Document Summarization\n",
    "\n",
    "Document summarization is a natural language processing task which involves creating a concise summary of the given text, while still capturing all the important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dY53sXf_ymwE"
   },
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "Coronaviruses are a large family of viruses which may cause illness in animals or humans.\n",
    "In humans, several coronaviruses are known to cause respiratory infections ranging from the\n",
    "common cold to more severe diseases such as Middle East Respiratory Syndrome (MERS) and Severe Acute Respiratory Syndrome (SARS).\n",
    "The most recently discovered coronavirus causes coronavirus disease COVID-19.\n",
    "COVID-19 is the infectious disease caused by the most recently discovered coronavirus.\n",
    "This new virus and disease were unknown before the outbreak began in Wuhan, China, in December 2019.\n",
    "COVID-19 is now a pandemic affecting many countries globally.\n",
    "The most common symptoms of COVID-19 are fever, dry cough, and tiredness.\n",
    "Other symptoms that are less common and may affect some patients include aches\n",
    "and pains, nasal congestion, headache, conjunctivitis, sore throat, diarrhea,\n",
    "loss of taste or smell or a rash on skin or discoloration of fingers or toes.\n",
    "These symptoms are usually mild and begin gradually.\n",
    "Some people become infected but only have very mild symptoms.\n",
    "Most people (about 80%) recover from the disease without needing hospital treatment.\n",
    "Around 1 out of every 5 people who gets COVID-19 becomes seriously ill and develops difficulty breathing.\n",
    "Older people, and those with underlying medical problems like high blood pressure, heart and lung problems,\n",
    "diabetes, or cancer, are at higher risk of developing serious illness.\n",
    "However, anyone can catch COVID-19 and become seriously ill.\n",
    "People of all ages who experience fever and/or  cough associated with difficulty breathing/shortness of breath,\n",
    "chest pain/pressure, or loss of speech or movement should seek medical attention immediately.\n",
    "If possible, it is recommended to call the health care provider or facility first,\n",
    "so the patient can be directed to the right clinic.\n",
    "People can catch COVID-19 from others who have the virus.\n",
    "The disease spreads primarily from person to person through small droplets from the nose or mouth,\n",
    "which are expelled when a person with COVID-19 coughs, sneezes, or speaks.\n",
    "These droplets are relatively heavy, do not travel far and quickly sink to the ground.\n",
    "People can catch COVID-19 if they breathe in these droplets from a person infected with the virus.\n",
    "This is why it is important to stay at least 1 meter) away from others.\n",
    "These droplets can land on objects and surfaces around the person such as tables, doorknobs and handrails.\n",
    "People can become infected by touching these objects or surfaces, then touching their eyes, nose or mouth.\n",
    "This is why it is important to wash your hands regularly with soap and water or clean with alcohol-based hand rub.\n",
    "Practicing hand and respiratory hygiene is important at ALL times and is the best way to protect others and yourself.\n",
    "When possible maintain at least a 1 meter distance between yourself and others.\n",
    "This is especially important if you are standing by someone who is coughing or sneezing.\n",
    "Since some infected persons may not yet be exhibiting symptoms or their symptoms may be mild,\n",
    "maintaining a physical distance with everyone is a good idea if you are in an area where COVID-19 is circulating.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7upxVwh5y8te"
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are an expert in generating accurate document summaries.\n",
    "Generate a summary of the given document.\n",
    "\n",
    "Document:\n",
    "{doc}\n",
    "\n",
    "\n",
    "Constraints: Please start the summary with the delimiter 'Summary'\n",
    "and limit the summary to 5 lines\n",
    "\n",
    "Summary:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hH8iyvU-6lw"
   },
   "source": [
    "Trying with Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "id": "tQwlDkgjg_4Q",
    "outputId": "ec681f48-4372-4ae4-9344-7e6d258e85c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Summary:\n",
       "Coronaviruses are a large family of viruses that cause respiratory infections in humans, ranging from the common cold to severe diseases like MERS and SARS. COVID-19 is a new infectious disease caused by a recently discovered coronavirus, first identified in Wuhan, China in December 2019. Common symptoms of COVID-19 include fever, dry cough, and tiredness, with some people experiencing mild symptoms. Older people and those with underlying medical conditions are at higher risk of developing serious illness. Practicing hand and respiratory hygiene, maintaining a 1-meter distance from others, and washing hands regularly can help prevent the spread of COVID-19."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion_llama(prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stOcUFS9g_4Q"
   },
   "source": [
    "Trying with GPT-4o mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "id": "ZKOhQRnsg_4Q",
    "outputId": "43870dbb-b8e2-46de-d33e-03bc6afedbad"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Summary: Coronaviruses are a large family of viruses that can cause respiratory infections in humans, including COVID-19, which emerged in December 2019. Common symptoms include fever, dry cough, and tiredness, with serious cases leading to difficulty breathing, particularly in older adults and those with underlying health issues. COVID-19 spreads primarily through respiratory droplets from infected individuals. Preventative measures include maintaining physical distance, practicing hand hygiene, and seeking medical attention for severe symptoms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = response = get_completion_gpt(prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keC5k5S-18cw"
   },
   "source": [
    "## Task 9: Transformation\n",
    "\n",
    "You can use LLMs to take an existing document and transform it into other formats of content and even generate training data for fine-tuning or training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ta182I2t2FD0"
   },
   "outputs": [],
   "source": [
    "fact_sheet_mobile = \"\"\"\n",
    "PRODUCT NAME\n",
    "Samsung Galaxy Z Fold4 5G Black\n",
    "​\n",
    "PRODUCT OVERVIEW\n",
    "Stands out. Stands up. Unfolds.\n",
    "The Galaxy Z Fold4 does a lot in one hand with its 15.73 cm(6.2-inch) Cover Screen.\n",
    "Unfolded, the 19.21 cm(7.6-inch) Main Screen lets you really get into the zone.\n",
    "Pushed-back bezels and the Under Display Camera means there's more screen\n",
    "and no black dot getting between you and the breathtaking Infinity Flex Display.\n",
    "Do more than more with Multi View. Whether toggling between texts or catching up\n",
    "on emails, take full advantage of the expansive Main Screen with Multi View.\n",
    "PC-like power thanks to Qualcomm Snapdragon 8+ Gen 1 processor in your pocket,\n",
    "transforms apps optimized with One UI to give you menus and more in a glance\n",
    "New Taskbar for PC-like multitasking. Wipe out tasks in fewer taps. Add\n",
    "apps to the Taskbar for quick navigation and bouncing between windows when\n",
    "you're in the groove.4 And with App Pair, one tap launches up to three apps,\n",
    "all sharing one super-productive screen\n",
    "Our toughest Samsung Galaxy foldables ever. From the inside out,\n",
    "Galaxy Z Fold4 is made with materials that are not only stunning,\n",
    "but stand up to life's bumps and fumbles. The front and rear panels,\n",
    "made with exclusive Corning Gorilla Glass Victus+, are ready to resist\n",
    "sneaky scrapes and scratches. With our toughest aluminum frame made with\n",
    "Armor Aluminum, this is one durable smartphone.\n",
    "World’s first water resistant foldable smartphones. Be adventurous, rain\n",
    "or shine. You don't have to sweat the forecast when you've got one of the\n",
    "world's first water-resistant foldable smartphones.\n",
    "​\n",
    "PRODUCT SPECS\n",
    "OS - Android 12.0\n",
    "RAM - 12 GB\n",
    "Product Dimensions - 15.5 x 13 x 0.6 cm; 263 Grams\n",
    "Batteries - 2 Lithium Ion batteries required. (included)\n",
    "Item model number - SM-F936BZKDINU_5\n",
    "Wireless communication technologies - Cellular\n",
    "Connectivity technologies - Bluetooth, Wi-Fi, USB, NFC\n",
    "GPS - True\n",
    "Special features - Fast Charging Support, Dual SIM, Wireless Charging, Built-In GPS, Water Resistant\n",
    "Other display features - Wireless\n",
    "Device interface - primary - Touchscreen\n",
    "Resolution - 2176x1812\n",
    "Other camera features - Rear, Front\n",
    "Form factor - Foldable Screen\n",
    "Colour - Phantom Black\n",
    "Battery Power Rating - 4400\n",
    "Whats in the box - SIM Tray Ejector, USB Cable\n",
    "Manufacturer - Samsung India pvt Ltd\n",
    "Country of Origin - China\n",
    "Item Weight - 263 g\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zko53u-N2KmD"
   },
   "outputs": [],
   "source": [
    "prompt =f\"\"\"Turn the following product description\n",
    "into a list of frequently asked questions (FAQ).\n",
    "Show both the question and its corresponding answer\n",
    "Generate at the max 5 but diverse and useful FAQs\n",
    "\n",
    "Product description:\n",
    "```{fact_sheet_mobile}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dA69wKk-70B"
   },
   "source": [
    "Trying with Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "AC4kbQP8hHVP",
    "outputId": "81d3ef41-4034-4b3f-e90b-6d18b88dc9cb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here are 5 diverse and useful FAQs based on the product description:\n",
       "\n",
       "**Q1: What is the screen size of the Galaxy Z Fold4 5G?**\n",
       "A1: The Galaxy Z Fold4 5G has a 15.73 cm (6.2-inch) Cover Screen and a 19.21 cm (7.6-inch) Main Screen when unfolded.\n",
       "\n",
       "**Q2: How durable is the Galaxy Z Fold4 5G?**\n",
       "A2: The Galaxy Z Fold4 5G is made with the toughest materials, including Corning Gorilla Glass Victus+ and Armor Aluminum, which provide excellent resistance to scratches and scrapes.\n",
       "\n",
       "**Q3: Can I use the Galaxy Z Fold4 5G in the rain?**\n",
       "A3: Yes, the Galaxy Z Fold4 5G is one of the world's first water-resistant foldable smartphones, so you can use it in the rain without worrying about damage.\n",
       "\n",
       "**Q4: What kind of multitasking capabilities does the Galaxy Z Fold4 5G offer?**\n",
       "A4: The Galaxy Z Fold4 5G offers PC-like multitasking capabilities with features like Multi View, New Taskbar, and App Pair, which allow you to easily switch between apps and windows.\n",
       "\n",
       "**Q5: What kind of processor powers the Galaxy Z Fold4 5G?**\n",
       "A5: The Galaxy Z Fold4 5G is powered by a Qualcomm Snapdragon 8+ Gen 1 processor, which provides fast performance and efficient battery life."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion_llama(prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXaM0yr4hHVQ"
   },
   "source": [
    "Trying with GPT-4o mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "9Mz0O9WDhHVQ",
    "outputId": "8c9e5f5f-4255-4dda-d401-a425123b0dbf"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Frequently Asked Questions (FAQ)\n",
       "\n",
       "**Q1: What are the main features of the Samsung Galaxy Z Fold4 5G?**  \n",
       "**A1:** The Samsung Galaxy Z Fold4 features a 6.2-inch Cover Screen and a 7.6-inch Main Screen with an Infinity Flex Display. It includes a Qualcomm Snapdragon 8+ Gen 1 processor, 12 GB of RAM, Multi View for multitasking, and a new Taskbar for easy navigation. It is also water-resistant and made with durable materials like Corning Gorilla Glass Victus+ and Armor Aluminum.\n",
       "\n",
       "---\n",
       "\n",
       "**Q2: Is the Samsung Galaxy Z Fold4 water-resistant?**  \n",
       "**A2:** Yes, the Samsung Galaxy Z Fold4 is one of the world’s first water-resistant foldable smartphones, allowing you to use it in various weather conditions without worry.\n",
       "\n",
       "---\n",
       "\n",
       "**Q3: What is the battery capacity of the Samsung Galaxy Z Fold4?**  \n",
       "**A3:** The Samsung Galaxy Z Fold4 is equipped with a battery power rating of 4400 mAh, and it supports fast charging, wireless charging, and dual SIM functionality.\n",
       "\n",
       "---\n",
       "\n",
       "**Q4: What operating system does the Samsung Galaxy Z Fold4 run on?**  \n",
       "**A4:** The Samsung Galaxy Z Fold4 runs on Android 12.0, optimized with One UI for enhanced performance and multitasking capabilities.\n",
       "\n",
       "---\n",
       "\n",
       "**Q5: What accessories are included in the box with the Samsung Galaxy Z Fold4?**  \n",
       "**A5:** The box includes a SIM Tray Ejector and a USB Cable."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = response = get_completion_gpt(prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53QdnJ0M3Vee"
   },
   "source": [
    "## Task 10: Translation\n",
    "\n",
    "You can use LLMs to take an existing document and translate it from a source to target language. You can also translate to multiple languages at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldlb_Avv3gHV"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are an expert translator.\n",
    "Translate the given text from English to German and Spanish.\n",
    "Show the output as key value pairs in JSON.\n",
    "Output should have all 3 languages.\n",
    "\n",
    "Text: 'Hello, how are you today?'\n",
    "Translation:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kVozsxz-9AK"
   },
   "source": [
    "Trying with Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "d-DTNPpchwVE",
    "outputId": "2ac45577-025c-4575-d10c-653d666fd533"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here is the translation of the given text in English to German and Spanish as key-value pairs in JSON format:\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"English\": \"Hello, how are you today?\",\n",
       "  \"German\": \"Hallo, wie geht es Ihnen heute?\",\n",
       "  \"Spanish\": \"Hola, ¿cómo estás hoy?\"\n",
       "}\n",
       "```\n",
       "\n",
       "Note: The translations are provided in a formal tone. If you need informal translations or any variations, please let me know!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion_llama(prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaCfXNlphwVE"
   },
   "source": [
    "Trying with GPT-4o mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "jdeyM11ShwVE",
    "outputId": "dc08f523-60f2-4f94-949f-a43ba7334467"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"English\": \"Hello, how are you today?\",\n",
       "  \"German\": \"Hallo, wie geht es dir heute?\",\n",
       "  \"Spanish\": \"Hola, ¿cómo estás hoy?\"\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = response = get_completion_gpt(prompt)\n",
    "display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "034d51e075b3496cac70aa337661a1e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b440fda4273e440eb998da140692001c",
      "placeholder": "​",
      "style": "IPY_MODEL_720ccfbcc77248a492c4ccd2336140b1",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "1fe3ec8ef7974c568bb570109eed81d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "27d8d80d4a1443e7a64e3687e2794bc7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ebcda8f035e441a84657ece09e7e909": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27d8d80d4a1443e7a64e3687e2794bc7",
      "placeholder": "​",
      "style": "IPY_MODEL_8d1e7f5a1b8241fa9d91ca438bbf08d2",
      "value": " 4/4 [00:06&lt;00:00,  1.41s/it]"
     }
    },
    "507a19ba74254128b60bf2ad989fd797": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d8ad2c43f324a4f9e80d2d619fe9294",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1fe3ec8ef7974c568bb570109eed81d1",
      "value": 4
     }
    },
    "720ccfbcc77248a492c4ccd2336140b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d1e7f5a1b8241fa9d91ca438bbf08d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d8ad2c43f324a4f9e80d2d619fe9294": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b440fda4273e440eb998da140692001c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0f900f8e0214e8c8285822d631177b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_034d51e075b3496cac70aa337661a1e0",
       "IPY_MODEL_507a19ba74254128b60bf2ad989fd797",
       "IPY_MODEL_2ebcda8f035e441a84657ece09e7e909"
      ],
      "layout": "IPY_MODEL_ec69e77dcaf34452a05b715e204115e0"
     }
    },
    "ec69e77dcaf34452a05b715e204115e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
