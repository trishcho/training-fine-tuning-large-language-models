{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS9QXHRoRiqc"
   },
   "source": [
    "# Text Representation with Word Embeddings\n",
    "\n",
    "### Exploring Word Embeddings with New Deep Learning Models\n",
    "\n",
    "We will explore more sophisticated models which can capture semantic information and give us features which are vector representation of words, popularly known as embeddings.\n",
    "\n",
    "Here we will explore the following feature engineering techniques:\n",
    "\n",
    "- Word2Vec\n",
    "\n",
    "Predictive methods like Neural Network based language models try to predict words from its neighboring words looking at word sequences in the corpus and in the process it learns distributed representations giving us dense word embeddings. We will be focusing on these predictive methods in this article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYUGSn3dRiqd"
   },
   "source": [
    "## Prepare a Sample Corpus\n",
    "\n",
    "Let’s now take a sample corpus of documents on which we will run most of our analyses in this article. A corpus is typically a collection of text documents usually belonging to one or more subjects or domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KH4ZwBgtRiqe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualize embeddings\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The birds love to eat and sleep and chirp',\n",
    "          'The cats love to eat and sleep',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog loves to sleep and eat'\n",
    "]\n",
    "\n",
    "\n",
    "corpus_df = pd.DataFrame({'Document': corpus})\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLojJzK0Riqi"
   },
   "source": [
    "Let's go ahead and pre-process our text data now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KR8w3qbRiqi"
   },
   "source": [
    "## Simple Text Pre-processing\n",
    "\n",
    "Since the focus of this unit is on feature engineering, we will build a simple text pre-processor which focuses on removing special characters, extra whitespaces, digits, stopwords and lower casing the text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YHHrL7VRiqj"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "# get list of common english stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "# download tokenizer - breaks down sentences into words\n",
    "nltk.download('punkt')\n",
    "\n",
    "# clean the text (not necessary too much in the case of deep learning models)\n",
    "def normalize_document(doc): # The sky is blue and beautiful......\n",
    "    # lower case and remove special characters\\ extra whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, flags=re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "norm_corpus = normalize_corpus(corpus)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MRWsCnGVyMi"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "tokenized_corpus = [nltk.word_tokenize(doc) for doc in norm_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BnB1TkIytRz"
   },
   "outputs": [],
   "source": [
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JCyCJ6GRiql"
   },
   "source": [
    "## The Word2Vec Model\n",
    "\n",
    "This model was created by Google in 2013 and is a predictive deep learning based model to compute and generate high quality, distributed and continuous dense vector representations of words, which capture contextual and semantic similarity. Essentially these are unsupervised models which can take in massive textual corpora, create a vocabulary of possible words and generate dense word embeddings for each word in the vector space representing that vocabulary.\n",
    "\n",
    "Usually you can specify the size of the word embedding vectors and the total number of vectors are essentially the size of the vocabulary. This makes the dimensionality of this dense vector space much lower than the high-dimensional sparse vector space built using traditional Bag of Words models.\n",
    "\n",
    "There are two different model architectures which can be leveraged by Word2Vec to create these word embedding representations. These include,\n",
    "\n",
    "- The Continuous Bag of Words (CBOW) Model\n",
    "- The Skip-gram Model\n",
    "\n",
    "## The Continuous Bag of Words (CBOW) Model\n",
    "\n",
    "The CBOW model architecture tries to predict the current target word (the center word) based on the source context words (surrounding words).\n",
    "\n",
    "Considering a simple sentence, ___“the quick brown fox jumps over the lazy dog”___, this can be pairs of __(context_window, target_word)__ where if we consider a context window of size 2, we have examples like __([quick, fox], brown)__, __([the, brown], quick)__, __([the, dog], lazy)__ and so on.\n",
    "\n",
    "Thus the model tries to predict the __`target_word`__ based on the __`context_window`__ words.\n",
    "\n",
    "![](https://i.imgur.com/ATyNx6u.png)\n",
    "\n",
    "\n",
    "## The Skip-gram Model\n",
    "\n",
    "The Skip-gram model architecture usually tries to achieve the reverse of what the CBOW model does. It tries to predict the source context words (surrounding words) given a target word (the center word).\n",
    "\n",
    "Considering our simple sentence from earlier, ___“the quick brown fox jumps over the lazy dog”___. If we used the CBOW model, we get pairs of __(context_window, target_word)__ where if we consider a context window of size 2, we have examples like __([quick, fox], brown)__, __([the, brown], quick)__, __([the, dog], lazy)__ and so on.\n",
    "\n",
    "Now considering that the skip-gram model’s aim is to predict the context from the target word, the model typically inverts the contexts and targets, and tries to predict each context word from its target word. Hence the task becomes to predict the context __[quick, fox]__ given target word __‘brown’__ or __[the, brown]__ given target word __‘quick’__ and so on.\n",
    "\n",
    "Thus the model tries to predict the context_window words based on the target_word.\n",
    "\n",
    "![](https://i.imgur.com/95f3eVF.png)\n",
    "\n",
    "Further details can be found in [Text Analytics with Python](https://github.com/dipanjanS/text-analytics-with-python/tree/master/New-Second-Edition/Ch04%20-%20Feature%20Engineering%20for%20Text%20Representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHD8pUx7Riqm"
   },
   "source": [
    "## Robust Word2Vec Model with Gensim\n",
    "\n",
    "The __`gensim`__ framework, created by Radim Řehůřek consists of a robust, efficient and scalable implementation of the Word2Vec model. We will leverage the same on our sample toy corpus. In our workflow, we will tokenize our normalized corpus and then focus on the following four parameters in the Word2Vec model to build it.\n",
    "\n",
    "- __`size`:__ The word embedding dimensionality\n",
    "- __`window`:__ The context window size\n",
    "- __`min_count`:__ The minimum word count\n",
    "- __`sample`:__ The downsample setting for frequent words\n",
    "- __`sg`:__ Training model, 1 for skip-gram otherwise CBOW\n",
    "\n",
    "We will build a simple Word2Vec model on the corpus and visualize the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYRXwquZvJle"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "gensim.__version__\n",
    "#4.0.0+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7JNogBHwbeu"
   },
   "outputs": [],
   "source": [
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_QVGSf5z_XW"
   },
   "source": [
    "__Replace `<REPLACE WITH CODE HERE>` sections with your own code__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqQIGvSSvBB3"
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 20    # Word vector dimensionality  every word -> [......] -> vector size of 20 float numbers\n",
    "window_context = 5  # Context window size (looking at surrounding words)\n",
    "min_word_count = 1   # Minimum word count\n",
    "sg = 1               # skip-gram model if sg = 1 and CBOW if sg = 0\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(<REPLACE WITH CODE HERE>)\n",
    "w2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-66cZn3z1Ap"
   },
   "source": [
    "## Exploring trained embeddings for words now\n",
    "\n",
    "__Replace `<REPLACE WITH CODE HERE>` sections with your own code__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBehsA9ewENL"
   },
   "outputs": [],
   "source": [
    "# embedding for the word sky\n",
    "w2v_model.wv['sky']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWtSDPkW2_GC"
   },
   "outputs": [],
   "source": [
    "# embedding for the word cats\n",
    "<REPLACE WITH CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmOr3lXv3DUN"
   },
   "outputs": [],
   "source": [
    "# embedding for the word dog\n",
    "<REPLACE WITH CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMnpltMJ_A-d"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv['sky'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frWwNrzNC52F"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NY4l-zaRiqo"
   },
   "outputs": [],
   "source": [
    "words = w2v_model.wv.index_to_key\n",
    "wvs = w2v_model.wv[words]\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "np.set_printoptions(suppress=True)\n",
    "pcs = pca.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(pcs[:, 1], pcs[:, 0], c='orange', edgecolors='r')\n",
    "for label, x, y in zip(labels, pcs[:, 1], pcs[:, 0]):\n",
    "    plt.annotate(label, xy=(x+0.005, y+0.005), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V7h2GmytRiqq"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv['sky'], w2v_model.wv['sky'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3INpkJK4Riqs"
   },
   "outputs": [],
   "source": [
    "vec_df = pd.DataFrame(wvs, index=words)\n",
    "vec_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTCiDguVRiqu"
   },
   "source": [
    "### Looking at term semantic similarity\n",
    "\n",
    "__Replace `<REPLACE WITH CODE HERE>` sections with your own code__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S63FJhi2Riqv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = <REPLACE WITH CODE HERE>\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=words, columns=words)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adi_Uo8szMHZ"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar('sky')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AqKFhpSkzO6a"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar('dog')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
